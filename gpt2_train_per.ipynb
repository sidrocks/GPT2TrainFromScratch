{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -q tiktoken transformers"
      ],
      "metadata": {
        "id": "DevNWhHbVyq5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjoCl_i8VfF9",
        "outputId": "00471827-7760-46f0-b29a-5a3d96253ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 330 batches\n",
            "loaded 338025 tokens\n",
            "1 epoch = 330 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: train loss 2.72210, val loss 10.11649, lr 0.000000\n",
            "Checkpoint saved.\n",
            "Step 25: train loss 0.81243, val loss 8.60325, lr 0.000037\n",
            "Checkpoint saved.\n",
            "Step 50: train loss 0.12097, val loss 8.26697, lr 0.000075\n",
            "Checkpoint saved.\n",
            "Step 75: train loss 0.01094, val loss 8.90390, lr 0.000112\n",
            "Step 100: train loss 0.00482, val loss 9.37317, lr 0.000150\n",
            "Step 125: train loss 0.00339, val loss 9.63627, lr 0.000188\n",
            "Step 150: train loss 0.00268, val loss 9.82545, lr 0.000225\n",
            "Step 175: train loss 0.00222, val loss 9.97914, lr 0.000262\n",
            "Step 200: train loss 0.00189, val loss 10.11232, lr 0.000300\n",
            "Step 225: train loss 0.00165, val loss 10.23065, lr 0.000300\n",
            "Step 250: train loss 0.00147, val loss 10.33702, lr 0.000299\n",
            "Step 275: train loss 0.00132, val loss 10.43411, lr 0.000299\n",
            "Step 300: train loss 0.00121, val loss 10.52248, lr 0.000298\n",
            "Step 325: train loss 0.00111, val loss 10.60463, lr 0.000297\n",
            "Step 350: train loss 0.00103, val loss 10.68145, lr 0.000295\n",
            "Step 375: train loss 0.00096, val loss 10.75374, lr 0.000293\n",
            "Step 400: train loss 0.00090, val loss 10.82089, lr 0.000291\n",
            "Step 425: train loss 0.00085, val loss 10.88519, lr 0.000289\n",
            "Step 450: train loss 0.00081, val loss 10.94615, lr 0.000286\n",
            "Step 475: train loss 0.00077, val loss 11.00458, lr 0.000284\n",
            "Step 500: train loss 0.00074, val loss 11.05933, lr 0.000281\n",
            "Step 525: train loss 0.00071, val loss 11.11200, lr 0.000277\n",
            "Step 550: train loss 0.00069, val loss 11.16200, lr 0.000274\n",
            "Early stopping triggered.\n",
            "> \n",
            "Women are made to bear, and so are you.\n",
            "\n",
            "KATHARINA:\n",
            "No such jade as you, if me you\n",
            ">  a buzzard.\n",
            "\n",
            "PETRUCHIO:\n",
            "O slow-wing'd turtle! shall a buzzard take thee?\n",
            "\n",
            "K\n",
            ">  find it where it lies,\n",
            "\n",
            "PETRUCHIO:\n",
            "Who knows not where a wasp does\n",
            "wear his sting? In his\n",
            "> UCHIO:\n",
            "I swear I'll cuff you, if you strike again.\n",
            "\n",
            "KATHARINA:\n",
            "So may you lose your\n",
            ">  you crow too like a craven.\n",
            "\n",
            "PETRUCHIO:\n",
            "Nay, come, Kate, come; you must not look\n"
          ]
        }
      ],
      "source": [
        "# Solving for residual std scaling issue\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.checkpoint import checkpoint # Moved this import to the top\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    # In forward of Block:\n",
        "    def forward(self, x):\n",
        "        def _forward_block(x):\n",
        "            x = x + self.attn(self.ln_1(x))\n",
        "            x = x + self.mlp(self.ln_2(x))\n",
        "            return x\n",
        "        return checkpoint(_forward_block, x)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 6 # number of layers (reduced from 12)\n",
        "    n_head: int = 6 # number of heads (reduced from 12)\n",
        "    n_embd: int = 384 # embedding dimension (reduced from 768)\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Device setup same as before\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Hyperparameters\n",
        "B, T = 8,128  # batch size and sequence length (8192 tokens per batch)\n",
        "max_iters = 2000\n",
        "warmup_iters = 200\n",
        "base_lr = 3e-4\n",
        "final_lr = 1e-5\n",
        "grad_clip = 1.0\n",
        "patience = 20  # early stopping patience\n",
        "num_val_batches = 10\n",
        "accum_steps = 4  # effectively batch 32 by accumulation\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # at init load tokens from disk and store them in memory\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "        # state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n",
        "# Load full tokens\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = torch.tensor(enc.encode(text))\n",
        "\n",
        "# Simple 90/10 train-val split to avoid data leakage\n",
        "num_train_tokens = int(0.9 * len(tokens))\n",
        "train_tokens = tokens[:num_train_tokens]\n",
        "val_tokens = tokens[num_train_tokens:]\n",
        "\n",
        "# Create data loaders pointing to split tokens\n",
        "train_loader = DataLoaderLite(B, T)\n",
        "train_loader.tokens = train_tokens\n",
        "train_loader.current_position = 0\n",
        "\n",
        "val_loader = DataLoaderLite(B, T)\n",
        "val_loader.tokens = val_tokens\n",
        "val_loader.current_position = 0\n",
        "\n",
        "# Clear CUDA cache before model initialization\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize model\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
        "\n",
        "# Learning rate schedule: linear warmup + cosine decay\n",
        "def get_lr(step):\n",
        "    if step < warmup_iters:\n",
        "        return base_lr * step / warmup_iters\n",
        "    progress = (step - warmup_iters) / (max_iters - warmup_iters)\n",
        "    return final_lr + 0.5 * (base_lr - final_lr) * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "no_improve_steps = 0\n",
        "train_losses = [] # Initialize train_losses list\n",
        "\n",
        "model.train()\n",
        "\n",
        "from torch.amp import GradScaler, autocast\n",
        "scaler = GradScaler('cuda')\n",
        "\n",
        "for step in range(max_iters):\n",
        "    optimizer.zero_grad()\n",
        "    for _ in range(accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            logits, loss = model(x, y)\n",
        "            loss = loss / accum_steps  # scale loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # Gradient clipping and optimizer step\n",
        "    scaler.unscale_(optimizer)\n",
        "    clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    train_losses.append(loss.item()) # Append training loss after each step\n",
        "\n",
        "  # Validation and logs every N steps (adjust for accum steps)\n",
        "    if step % (100 // accum_steps) == 0 or step == max_iters - 1:\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_val_batches):\n",
        "                xv, yv = val_loader.next_batch()\n",
        "                xv, yv = xv.to(device), yv.to(device)\n",
        "                _, val_loss = model(xv, yv)\n",
        "                val_losses.append(val_loss.item())\n",
        "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "\n",
        "        lr = get_lr(step) # Assign the learning rate\n",
        "        print(f\"Step {step}: train loss {train_losses[-1]:.5f}, val loss {avg_val_loss:.5f}, lr {lr:.6f}\") # Use train_losses[-1]\n",
        "\n",
        "        # Early stopping and checkpoint saving\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            no_improve_steps = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            print(\"Checkpoint saved.\")\n",
        "        else:\n",
        "            no_improve_steps += 1\n",
        "            if no_improve_steps >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "        model.train()\n",
        "\n",
        "# Load best model for sampling/generation\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "model.eval()\n",
        "\n",
        "# Sampling/generation code (unchanged from original)\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "x = val_loader.next_batch()[0][:num_return_sequences].to(device)  # start from some validation tokens\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)[0]\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        ix = torch.multinomial(topk_probs, 1)\n",
        "        xcol = torch.gather(topk_indices, -1, ix)\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ]
    }
  ]
}